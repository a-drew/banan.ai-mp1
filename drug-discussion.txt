Q:

Does the same model give you the same performance every time? Explain

A:

The Gaussian Naive Bayes and decision tree methods returned the same results for each iteration. The only 3 whose iterations produced different results were the Perceptron variations (i.e. simple, multi-layered, base multi-layered). This is because the former methods make some fundamentally simplifiying assumptions such as the interdependence of all features and their probabilities. While this reduces the complexity of the tasks and speeds up processing time, on the other hand the latter methods (i.e. perceptrons) make their decisions based on multiple different inputs while considering their weights and corresponding thresholds.

We begin to see the first deviation from an otherwise perfect 0 value with the simple Perceptron. Since perceptrons are realistically used in multiples, this example proves rather unsatisfactory with a weighted F1 score of about 54%. Yet, there is still hope, for as we increase the amount of perceptrons (i.e. neurons) employed, we see a substantial increase of accuracy up to around 72% for the base MLP and then up to 89% for the top MLP.

It was found through testing that the maximum iterations had to be set to 4000 (assumed for discussion sake) to ensure all MLP models would eventually converge on their results. In this case the training dataset was not as balanced as for task 1 (with extremes at 45% for drug Y vs drug B and C at 8% each), therefore the results were more biased in favor of drug Y for all models used.
