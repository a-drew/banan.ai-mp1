Q:

Does the same model give you the same performance every time? Explain

A:

The Gaussian Naive Bayes and decision tree methods returned the same results for each iteration. The only 3 whose iterations produced different results were the Perceptron variations (i.e. simple, multi-layered, base multi-layered). This is because the former methods make some fundamentally simplifiying assumptions such as the interdependence of all features and their probabilities. While this reduces the complexity of the tasks and speeds up processing time, on the other hand the latter methods (i.e. perceptrons) make their decisions based on multiple different inputs while considering their weights and corresponding thresholds.

We begin to see the first deviation from an otherwise perfect 0 value with the simple Perceptron. Since perceptrons are realistically used in multiples, this example proves rather unsatisfactory with low accuracy score of about 50%. Yet, there is hope, for as we increase the amount of perceptrons (i.e. neurons) employed we see a drastic increase of accuraicy up to around 69% for the base MLP and then up to 95% for the top MLP.

It was found through testing that the maximum iterations had to be set to 4000 (assumed for discussion sake) to ensure all MLP models would eventually converge on their results.
