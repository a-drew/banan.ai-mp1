===================
 Naive Bayes Try 1
===================

(b) confusion_matrix:
[[93  0  2  0  1]
 [ 0 79  1  0  2]
 [ 1  0 87  0  0]
 [ 0  0  0 97  0]
 [ 0  0  0  0 82]]

(c/d) classification_report: 
               precision    recall  f1-score   support

     business       0.99      0.97      0.98        96
entertainment       1.00      0.96      0.98        82
     politics       0.97      0.99      0.98        88
        sport       1.00      1.00      1.00        97
         tech       0.96      1.00      0.98        82

     accuracy                           0.98       445
    macro avg       0.98      0.98      0.98       445
 weighted avg       0.98      0.98      0.98       445


(e) prior probability of each class: 
{'business': 0.23258426966292134, 'entertainment': 0.1707865168539326, 'politics': 0.18483146067415732, 'sport': 0.23258426966292134, 'tech': 0.17921348314606741}

(f) vocabulary size: 29421

(g) word-tokens in each class: 

(h) word-tokens in the entire corpus: 

(i) the number and percentage of words with a frequency of zero in each class: 

(j) the number and percentage of words with a frequency of one in the entire corpus: 

(k) your 2 favorite words (that are present in the vocabulary) and their log-prob: 


===================
 Naive Bayes Try 2
===================

(b) confusion_matrix:
[[93  0  2  0  1]
 [ 0 79  1  0  2]
 [ 1  0 87  0  0]
 [ 0  0  0 97  0]
 [ 0  0  0  0 82]]

(c/d) classification_report: 
               precision    recall  f1-score   support

     business       0.99      0.97      0.98        96
entertainment       1.00      0.96      0.98        82
     politics       0.97      0.99      0.98        88
        sport       1.00      1.00      1.00        97
         tech       0.96      1.00      0.98        82

     accuracy                           0.98       445
    macro avg       0.98      0.98      0.98       445
 weighted avg       0.98      0.98      0.98       445


(e) prior probability of each class: 
{'business': 0.23258426966292134, 'entertainment': 0.1707865168539326, 'politics': 0.18483146067415732, 'sport': 0.23258426966292134, 'tech': 0.17921348314606741}

(f) vocabulary size: 29421

(g) word-tokens in each class: 

(h) word-tokens in the entire corpus: 

(i) the number and percentage of words with a frequency of zero in each class: 

(j) the number and percentage of words with a frequency of one in the entire corpus: 

(k) your 2 favorite words (that are present in the vocabulary) and their log-prob: 


===================
 Naive Bayes Try 3
===================

(b) confusion_matrix:
[[93  0  2  0  1]
 [ 1 79  1  0  1]
 [ 1  0 87  0  0]
 [ 1  0  0 96  0]
 [ 1  0  0  0 81]]

(c/d) classification_report: 
               precision    recall  f1-score   support

     business       0.96      0.97      0.96        96
entertainment       1.00      0.96      0.98        82
     politics       0.97      0.99      0.98        88
        sport       1.00      0.99      0.99        97
         tech       0.98      0.99      0.98        82

     accuracy                           0.98       445
    macro avg       0.98      0.98      0.98       445
 weighted avg       0.98      0.98      0.98       445


(e) prior probability of each class: 
{'business': 0.23258426966292134, 'entertainment': 0.1707865168539326, 'politics': 0.18483146067415732, 'sport': 0.23258426966292134, 'tech': 0.17921348314606741}

(f) vocabulary size: 29421

(g) word-tokens in each class: 

(h) word-tokens in the entire corpus: 

(i) the number and percentage of words with a frequency of zero in each class: 

(j) the number and percentage of words with a frequency of one in the entire corpus: 

(k) your 2 favorite words (that are present in the vocabulary) and their log-prob: 


===================
 Naive Bayes Try 4
===================

(b) confusion_matrix:
[[93  0  2  0  1]
 [ 0 79  1  0  2]
 [ 1  0 87  0  0]
 [ 0  0  0 97  0]
 [ 0  0  0  0 82]]

(c/d) classification_report: 
               precision    recall  f1-score   support

     business       0.99      0.97      0.98        96
entertainment       1.00      0.96      0.98        82
     politics       0.97      0.99      0.98        88
        sport       1.00      1.00      1.00        97
         tech       0.96      1.00      0.98        82

     accuracy                           0.98       445
    macro avg       0.98      0.98      0.98       445
 weighted avg       0.98      0.98      0.98       445


(e) prior probability of each class: 
{'business': 0.23258426966292134, 'entertainment': 0.1707865168539326, 'politics': 0.18483146067415732, 'sport': 0.23258426966292134, 'tech': 0.17921348314606741}

(f) vocabulary size: 29421

(g) word-tokens in each class: 

(h) word-tokens in the entire corpus: 

(i) the number and percentage of words with a frequency of zero in each class: 

(j) the number and percentage of words with a frequency of one in the entire corpus: 

(k) your 2 favorite words (that are present in the vocabulary) and their log-prob: 


